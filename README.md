A lightweight Flask + FAISS + OCR + Mistral (Ollama) project that allows you to:

âœ” Upload a PDF document
âœ” Extract text (page-wise)
âœ” Chunk it
âœ” Embed using Sentence-Transformers
âœ” Store embeddings in a FAISS vector store
âœ” Query using RAG
âœ” Use Mistral (Ollama locally) to generate answers
âœ” Highlight legal sections found in retrieved text

ğŸš€ 1. Project Overview

This is a RAG (Retrieval-Augmented Generation) pipeline designed for legal document question answering.

When a user uploads a PDF:

The PDF is saved in /uploads.

The text is extracted (OCR handled automatically by PyMuPDF/fitz).

The text is chunked into small pieces.

Each chunk is embedded using Sentence-Transformers (MiniLM).

A FAISS vector index is created and stored in /indexes.

Metadata (chunk text + page number) is saved in JSON.

When the user asks a question:

System loads the FAISS index.

Converts the question into an embedding.

Searches top-k most relevant chunks.

Extracts legal sections using regex patterns.

Sends the RAG context + user question to Mistral (via Ollama).

Displays generated answer, relevant pages, and legal sections.

ğŸ— 2. Folder Structure
project/
â”‚â”€â”€ app.py                # Main Flask application
â”‚â”€â”€ templates/
â”‚    â””â”€â”€ index.html       # UI
â”‚â”€â”€ uploads/              # Uploaded PDFs
â”‚â”€â”€ indexes/              # FAISS index + metadata
â”‚â”€â”€ static/               # CSS/JS (optional)

âš™ï¸ 3. How the Pipeline Works (Simple Explanation)
STEP 1 â€” Upload & Save PDF

The file is uploaded via /upload.
It gets saved as:

uploads/<filename>.pdf


A unique ID is appended to avoid collisions.

STEP 2 â€” Extract Text from PDF

Using PyMuPDF:

doc = fitz.open(pdf_path)
page.get_text()


Each page â†’ stored as {page_number, text}.

STEP 3 â€” Chunking

Large text is broken into smaller chunks (500 tokens each):

chunk1
chunk2
chunk3
...


This helps better retrieval.

STEP 4 â€” Embeddings

Every chunk is converted into a 384-dim vector using:

sentence-transformers/all-MiniLM-L6-v2


These embeddings are stored in a FAISS index.

STEP 5 â€” Save FAISS Index + Metadata

Two files are written:

indexes/doc_index.faiss
indexes/doc_meta.json


meta.json stores mapping:

[
  {
    "page": 1,
    "text": "Chunk summary..."
  }
]

ğŸ” 4. Querying (Ask a Question)
STEP 6 â€” Retrieve Relevant Chunks

When a query is asked:

Convert the query to embedding

Search top-k similar vectors in FAISS

Fetch corresponding chunk texts from metadata

Combine these into a reference context

STEP 7 â€” Extract Legal Sections

Regex patterns detect:

â€œSection 420â€

â€œU/s 302â€

â€œSec 125â€

â€œ304 IPCâ€

etc.

STEP 8 â€” Generate Final Answer (LLM)

Prompt is formed:

CONTEXT:
<retrieved chunks>

QUESTION:
<user question>


This is sent to Ollama:

http://localhost:11434/api/generate


Streaming is supported.

STEP 9 â€” Display on UI

UI shows:

âœ” Final answer
âœ” Pages used
âœ” Extracted legal sections
âœ” Previously indexed documents
